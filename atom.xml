<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>KK&#39;s Notes</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://jkknotes.com/"/>
  <updated>2019-04-07T12:35:19.810Z</updated>
  <id>http://jkknotes.com/</id>
  
  <author>
    <name>KK.J</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hello World</title>
    <link href="http://jkknotes.com/Life/hello-world.html"/>
    <id>http://jkknotes.com/Life/hello-world.html</id>
    <published>2019-04-07T12:35:19.810Z</published>
    <updated>2019-04-07T12:35:19.810Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.<br><a id="more"></a></p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new &quot;My New Post&quot;</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;br&gt;
    
    </summary>
    
      <category term="Life" scheme="http://jkknotes.com/categories/Life/"/>
    
    
      <category term="study" scheme="http://jkknotes.com/tags/study/"/>
    
      <category term="tool" scheme="http://jkknotes.com/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>kaggle实战——What Causes Heart Disease?</title>
    <link href="http://jkknotes.com/Machine-Learning/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/kaggle%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94What-Causes-Heart-Disease.html"/>
    <id>http://jkknotes.com/Machine-Learning/项目实战/kaggle实战——What-Causes-Heart-Disease.html</id>
    <published>2019-04-07T12:29:29.000Z</published>
    <updated>2019-04-07T13:19:38.655Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><strong>记得有一次去面试，那个公司的HR聊天说，她感觉程序员面试那是面真功夫，会就会，不会装也没用。从这里想开来，还真是，码农学再多理论，终究是要去码砖的。我呢就是原来机器学习和深度学习的理论学的多，实践反而少，所以感觉有时候做事情就慢了些。现在趁着还有些闲工夫，就找一些项目做做，由简单到复杂，慢慢来吧。</strong><br><a id="more"></a></p></blockquote><p>本文同步发在我的<a href="https://blog.csdn.net/iizhuzhu/article/details/89067386" target="_blank" rel="noopener">CSDN Blog</a>，接下来CSDN 和 KK’s Notes 同步更新，各位看官大佬多多指教。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>这个项目来自于<a href="https://www.kaggle.com/tentotheminus9/what-causes-heart-disease-explaining-the-model/notebook" target="_blank" rel="noopener">kaggle</a>。项目主要是利用患者的个人信息和检查数据，利用机器学习方法来诊断该患者收否患疾病，并且尝试对识别结果作出解释。这个项目虽然简单但将机器学习的全流程和常用预处理和分析方法都涉及到了，我做完一遍还是有很多收获。以下操作皆在 <strong>jubyter notebook</strong> 下以 <strong>python</strong> 进行的。</p><p>主要使用的技术：</p><ul><li>Random Forest</li><li>Feature Importance Analysis: <strong>Permutation importance</strong></li><li>Feature Importance Analysis: <strong>Partial Dependence Plots</strong></li></ul><h2 id="2-Data"><a href="#2-Data" class="headerlink" title="2. Data"></a>2. Data</h2><p>Data from：<a href="https://www.kaggle.com/ronitf/heart-disease-uci/downloads/heart.csv/" target="_blank" rel="noopener">https://www.kaggle.com/ronitf/heart-disease-uci/downloads/heart.csv/</a><br>About Data：下载好数据之后直接打开看一看。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">data = pd.read_csv(&apos;data/heart.csv&apos;)</span><br><span class="line">data.info()</span><br></pre></td></tr></table></figure><p>Output:<br><img src="https://img-blog.csdnimg.cn/2019040713094932.png" width="342" height="334"><br>可以看到总共有303条数据以及13个特征和1个标签，数据没有缺失项。接下看下前十个数据。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.head(10)</span><br></pre></td></tr></table></figure></p><p>Output:<br><img src="https://img-blog.csdnimg.cn/20190407125529632.png" width="621" height="308"><br>这13个特征的含义分别是：</p><blockquote><p>age: 年龄<br>sex：该人的性别（1=男性，0=女性）<br>cp：胸痛经历（值1：典型心绞痛，值2：非典型心绞痛，值3：非心绞痛，值4：无症状）<br>trestbps：该人的静息血压（入院时为mm Hg）<br>chol：人体胆固醇测量单位为mg/dl<br>fbs：该人的空腹血糖（&gt; 120mg/dl，1=true; 0= f=alse）<br>restecg：静息心电图测量（0=正常，1=有ST-T波异常，2=按Estes标准显示可能或明确的左心室肥厚）<br>thalach：达到了该人的最大心率<br>exang：运动诱发心绞痛（1=是; 0=否）<br>oldpeak：运动相对于休息引起的ST段压低（’ST’与ECG图上的位置有关）<br>slope：峰值运动ST段的斜率（值1：上升，值2：平坦，值3：下降）<br>ca：主要血管数量（0-3）<br>thal：称为地中海贫血的血液疾病（1=正常; 2=固定缺陷; 3=可逆缺陷）<br>target：心脏病（0=不，1=是）</p></blockquote><p>为了更好的理解数据，我们应该提前查一下每个特征的含义，以及医学上该特征和心脏病的关系。具体这里不再赘述。</p><h2 id="3-数据预处理"><a href="#3-数据预处理" class="headerlink" title="3. 数据预处理"></a>3. 数据预处理</h2><p>这里为了方便后续做心脏病诊断中影响因素分析即Feature Importance Analysis（还是觉得用英文更能表达意思），将部分数值型特征进行转换：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">data.loc[data.sex == 1, &apos;sex&apos;] = &apos;male&apos;</span><br><span class="line">data.loc[data[&apos;sex&apos;] == 0, &apos;sex&apos;] = &apos;female&apos;</span><br><span class="line"></span><br><span class="line">data.loc[data[&apos;cp&apos;] == 1, &apos;cp&apos;] = &apos;typical&apos;</span><br><span class="line">data.loc[data[&apos;cp&apos;] == 2, &apos;cp&apos;] = &apos;atypical&apos;</span><br><span class="line">data.loc[data[&apos;cp&apos;] == 3, &apos;cp&apos;] = &apos;no_pain&apos;</span><br><span class="line">data.loc[data[&apos;cp&apos;] == 4, &apos;cp&apos;] = &apos;no_feel&apos;</span><br><span class="line"></span><br><span class="line">data.loc[data[&apos;fbs&apos;] == 1, &apos;fbs&apos;] = &apos;higher than 120 mg/dl&apos;</span><br><span class="line">data.loc[data[&apos;fbs&apos;] == 0, &apos;fbs&apos;] = &apos;lower than 120 mg/dl&apos;</span><br><span class="line"></span><br><span class="line">data.loc[data[&apos;restecg&apos;] == 0, &apos;restecg&apos;] = &apos;normal&apos;</span><br><span class="line">data.loc[data[&apos;restecg&apos;] == 1, &apos;restecg&apos;] = &apos;ST-T wave abnormality&apos;</span><br><span class="line">data.loc[data[&apos;restecg&apos;] == 2, &apos;restecg&apos;] = &apos;left ventricular hypertrophy&apos;</span><br><span class="line"></span><br><span class="line">data.loc[data[&apos;exang&apos;] == 1, &apos;exang&apos;] = &apos;true&apos;</span><br><span class="line">data.loc[data[&apos;exang&apos;] == 0, &apos;exang&apos;] = &apos;false&apos;</span><br><span class="line"></span><br><span class="line">data.loc[data[&apos;slope&apos;] == 1, &apos;slope&apos;] = &apos;up&apos;</span><br><span class="line">data.loc[data[&apos;slope&apos;] == 2, &apos;slope&apos;] = &apos;flat&apos;</span><br><span class="line">data.loc[data[&apos;slope&apos;] == 3, &apos;slope&apos;] = &apos;down&apos;</span><br><span class="line"></span><br><span class="line">data.loc[data[&apos;thal&apos;] == 1, &apos;thal&apos;] = &apos;normal&apos;</span><br><span class="line">data.loc[data[&apos;thal&apos;] == 2, &apos;thal&apos;] = &apos;fixed defect&apos;</span><br><span class="line">data.loc[data[&apos;thal&apos;] == 3, &apos;thal&apos;] = &apos;reversable defect&apos;</span><br></pre></td></tr></table></figure></p><p>检查下数据情况：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.describe(include=[np.object])</span><br></pre></td></tr></table></figure><p>Output:<br><img src="https://img-blog.csdnimg.cn/20190407141414294.png" width="560" height="147"><br>可以看到特征thal有4个值，而我们在转换时只转换了3个。实际上thal存在2个缺失值用0补齐的。为了防止数据类型错误，这里做一下类型转换。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[&apos;thal&apos;] = data[&apos;thal&apos;].astype(&apos;object&apos;)</span><br></pre></td></tr></table></figure><p>再看下数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.head()</span><br></pre></td></tr></table></figure></p><p>Output:<br><img src="https://img-blog.csdnimg.cn/20190407133939266.png" width="889" height="174"><br>模型的训练肯定需要数值型特征。这里对特征进行Onehot编码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = pd.get_dummies(data, drop_first=True)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://img-blog.csdnimg.cn/20190407142830450.png" width="1011" height="186"><br>（由于我还不知道在用markdown编辑时怎么显示运行结果，这里用的是截图，只能截取一部分，还有特征没有截取出来）<br>数据预处理部分就到此为止，接下来上模型。</p><h2 id="4-Random-Forest"><a href="#4-Random-Forest" class="headerlink" title="4. Random Forest"></a>4. Random Forest</h2><p>对于 Random Forest 的原理这里就不介绍了，网上介绍的文章也很多。废话不多说，直接import package.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br></pre></td></tr></table></figure><p>将数据分成 train_data 和 test_data 2个集合，二者比例为8:2。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_x, test_x, train_y, test_y = train_test_split(data.drop(columns=&apos;target&apos;),</span><br><span class="line">                                                    data[&apos;target&apos;],</span><br><span class="line">                                                    test_size=0.2,</span><br><span class="line">                                                    random_state=10)</span><br></pre></td></tr></table></figure><p>简单的画个图调个参。这里 Random Forest 主要的参数有基学习器决策树的最大深度（这里依据经验选5）、基学习器个数 n_estimators。这里基学习器选用CART。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">train_score = []</span><br><span class="line">test_score = []</span><br><span class="line"></span><br><span class="line">for n in range(1, 100):</span><br><span class="line">    model = RandomForestClassifier(max_depth=5,</span><br><span class="line">                                   n_estimators=n，</span><br><span class="line">                                   criterion=&apos;gini&apos;)</span><br><span class="line">    model.fit(train_x, train_y)</span><br><span class="line">    train_score.append(model.score(train_x, train_y))</span><br><span class="line">    test_score.append(model.score(test_x, test_y))</span><br></pre></td></tr></table></figure><p>训练完，把train和test上的accuracy随基学习器个数的变化画成图。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x_axis = [i for i in range(1, 100)]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(x_axis, train_score[:99])</span><br><span class="line">ax.plot(x_axis, test_score[:99], c=&quot;r&quot;)</span><br><span class="line">plt.xlim([0, 100])</span><br><span class="line">plt.ylim([0.0, 1.0])</span><br><span class="line">plt.rcParams[&apos;font.size&apos;] = 12</span><br><span class="line">plt.xlabel(&apos;n_estimators&apos;)</span><br><span class="line">plt.ylabel(&apos;accuracy&apos;)</span><br><span class="line">plt.grid(True)</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://img-blog.csdnimg.cn/20190407152446630.png" alt><br>可以看到大概是n_estimators=14的时候效果最好，train和test上的accuracy分别是0.9463，0.8361。看上去没有那么差。</p><h2 id="5-模型评估"><a href="#5-模型评估" class="headerlink" title="5. 模型评估"></a>5. 模型评估</h2><p>训练完模型，用<a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank" rel="noopener">ROC曲线</a>来评估下模型的效果。ROC曲线事宜FPR和TPR分别为横纵轴作出的曲线，其和坐标轴围成的面积越大，说明模型效果越好。具体评判标准见下文。说一下几个概念：</p><blockquote><ul><li>TPR: 真正例率，表示所有真正为正例的样本被正确预测出来的比例，等同于Recall</li><li>FNR: 假负例率，FNR = 1 - TPR</li><li>FPR: 假正例率，表示所有负例中被预测为正例的比例。</li><li>TNR: 真负例率，TNR = 1 - FPR</li></ul></blockquote><p>好吧，我也快晕了。<br>接下来计算一下正例和负例的recall</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import confusion_matrix</span><br><span class="line">from sklearn.metrics import auc, roc_curve</span><br><span class="line"></span><br><span class="line"># 混淆矩阵</span><br><span class="line">confusion_m = confusion_matrix(test_y, pred_y) </span><br><span class="line">print confusion_m</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[29  6]</span><br><span class="line"> [ 4 22]]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total = confusion_m.sum()</span><br><span class="line">tpr = float(confusion_m[0][0]) / (confusion_m[0][0] + confusion_m[1][0])</span><br><span class="line">tnr = float(confusion_m[1][1]) / (confusion_m[1][1] + confusion_m[0][1])</span><br><span class="line">print tpr, tnr</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.878787878788 0.785714285714</span><br></pre></td></tr></table></figure><p>Just so so!!</p><p>画ROC曲线图：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pred_y = model.predict(test_x)  # 预测结果</span><br><span class="line">pred_prob_y = model.predict_proba(test_x)[:, 1]  # 为正例的概率</span><br><span class="line">fpr_list, tpr_list, throsholds = roc_curve(test_y, pred_prob_y)</span><br><span class="line"></span><br><span class="line"># 画图</span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(fpr_list, tpr_list)</span><br><span class="line">ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=&quot;--&quot;, c=&quot;r&quot;)</span><br><span class="line">plt.xlim([0.0, 1.0])</span><br><span class="line">plt.ylim([0.0, 1.0])</span><br><span class="line">plt.rcParams[&apos;font.size&apos;] = 12</span><br><span class="line">plt.title(&apos;roc curve&apos;)</span><br><span class="line">plt.xlabel(&apos;fpr&apos;)</span><br><span class="line">plt.ylabel(&apos;tpr&apos;)</span><br><span class="line">plt.grid(True)</span><br></pre></td></tr></table></figure><p>Output:<br><img src="https://img-blog.csdnimg.cn/20190407163450571.png" alt><br>前文说了，ROC曲线和坐标轴围成的面积越大，说明模型效果越好。这个面积就叫 AUC .根据AUC的值，可参考下面的规则评估模型：</p><blockquote><ul><li>0.90 - 1.00 = excellent</li><li>0.80 - 0.90 = good</li><li>0.70 - 0.80 = fair</li><li>0.60 - 0.70 = poor</li><li>0.50 - 0.60 = fail</li></ul></blockquote><p>看看我们训练模型的AUC</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">auc(fpr_list, tpr_list)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9032967032967033</span><br></pre></td></tr></table></figure><p>OK， working well！<br><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1554636629238&amp;di=b77d71e635f8ed2e7fa40c847ac1b893&amp;imgtype=0&amp;src=http://b-ssl.duitang.com/uploads/item/201703/29/20170329161728_fdSMF.thumb.224_0.gif" alt></p><h2 id="6-Feature-Importance-Analysis"><a href="#6-Feature-Importance-Analysis" class="headerlink" title="6. Feature Importance Analysis"></a>6. Feature Importance Analysis</h2><p>训练完模型，我们希望能从模型里得到点什么， 比如说哪些特征对模型结果贡献率比较大，是不是意味着这些影响因素在实际心脏病诊断中也是很重要对参考，或者说还能发现一些现有医学没有发现的发现。所有接下来我们做的是一件很有意思的事。</p><h5 id="6-1-决策树可视化"><a href="#6-1-决策树可视化" class="headerlink" title="6.1 决策树可视化"></a>6.1 决策树可视化</h5><p>如果我没记错的话， 根据决策树的原理，越先分裂的特征越重要。那么下面对决策树进行可视化，看看它到底做了什么。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import export_graphviz</span><br><span class="line"></span><br><span class="line"># 输出 feature_name</span><br><span class="line">estimator = model.estimators_[1]</span><br><span class="line">features = [i for i in train_x.columns]</span><br><span class="line"></span><br><span class="line"># 0 —&gt; no disease，1 —&gt; disease</span><br><span class="line">train_y_str = train_y.astype(&apos;str&apos;)</span><br><span class="line">train_y_str[train_y_str == &apos;0&apos;] = &apos;no disease&apos;</span><br><span class="line">train_y_str[train_y_str == &apos;1&apos;] = &apos;disease&apos;</span><br><span class="line">train_y_str = train_y_str.values</span><br></pre></td></tr></table></figure><p>sklearn 真是个好东西，你能想到对功能他都有。下面用 sklearn 的 export_graphviz 对决策树进行可视化。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export_graphviz(estimator, out_file=&apos;tree.dot&apos;, </span><br><span class="line">                feature_names = features,</span><br><span class="line">                class_names = train_y_str,</span><br><span class="line">                rounded = True, proportion = True, </span><br><span class="line">                label=&apos;root&apos;,</span><br><span class="line">                precision = 2, filled = True)</span><br></pre></td></tr></table></figure><p>生成对这个 tree.dot 文件还不能直接看，网上查了一下，把它输出来看看。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import pydotplus</span><br><span class="line">from IPython.display import Image</span><br><span class="line">img = pydotplus.graph_from_dot_file(&apos;tree.dot&apos;)</span><br><span class="line">#img.write_pdf(&apos;tree.pdf&apos;) #输出成PDF</span><br><span class="line">Image(img.create_png())</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://img-blog.csdnimg.cn/20190407154755479.png" alt><br>实际上这张图就解释来决策树的生成过程。一般我们认为最先分裂的特征越重要，但是从这张图我们并不能很直观的看出特征的重要性。</p><h5 id="6-2-Permutation-importance"><a href="#6-2-Permutation-importance" class="headerlink" title="6.2 Permutation importance"></a>6.2 Permutation importance</h5><p>我们换一个工具—<a href="https://www.kaggle.com/dansbecker/permutation-importance" target="_blank" rel="noopener">Permutation importance</a>. 其原理是依次打乱test_data中其中一个特征数值的顺序，其实就是做shuffle，然后观察模型的效果，下降的多的说明这个特征对模型比较重要。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import eli5</span><br><span class="line">from eli5.sklearn import PermutationImportance</span><br><span class="line"></span><br><span class="line">perm = PermutationImportance(model, random_state=20).fit(test_x, test_y)</span><br><span class="line">eli5.show_weights(perm, feature_names=test_x.columns.tolist())</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://img-blog.csdnimg.cn/20190407171736186.png" width="323" height="290"><br>一目了然，一切尽在不言中。还是说俩句吧，绿色越深表示正相关越强，红色越深表示负相关越强。<br>实际上我发现改变 PermutationImportance 的参数 random_state 的值结果变化挺大的，不过还是有几个特征位次变化不大，结果还是具有参考意义。</p><h5 id="6-3-Partial-Dependence-Plots"><a href="#6-3-Partial-Dependence-Plots" class="headerlink" title="6.3 Partial Dependence Plots"></a>6.3 Partial Dependence Plots</h5><p>我们试试另一个工具—<a href="https://www.kaggle.com/dansbecker/partial-plots" target="_blank" rel="noopener">Partial Dependence Plots</a>. 其原理和 Permutation importance 有点类似，当它判断一个特征对模型的影响时，对于所有样本，将该特征依次取该特征的所有取值，观察模型结果的变化。先画图，再根据图解释一下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from pdpbox import pdp, info_plots</span><br><span class="line"></span><br><span class="line">total_features = train_x.columns.values.tolist()</span><br><span class="line">feature_name = &apos;oldpeak&apos;</span><br><span class="line">pdp_dist = pdp.pdp_isolate(model=model, dataset=test_x, model_features=total_features, feature=feature_name)</span><br><span class="line"></span><br><span class="line">pdp.pdp_plot(pdp_dist, feature_name)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://img-blog.csdnimg.cn/20190407190043782.png" width="670" height="425"><br>上图的纵坐标是模型相对于base model 的变化，横坐标是该特征的所有取值，实线表示相对于base model 的变化的平均值，蓝色阴影表示置信度。oldpeak表示运动相对于休息引起的ST段压低，可以看到其取值越大，患心脏病的可能性越低。不知道这个结果可不可信，我觉得需要医学知识作支撑。</p><p>又试了几个特征：</p><p><strong>Sex：</strong><br><img src="https://img-blog.csdnimg.cn/20190407190816418.png" width="670" height="425"><br>上图说明男性比女性患心脏病的概率要低些，网上查了一下，还真是这样。</p><p><strong>Age：</strong><br><img src="https://img-blog.csdnimg.cn/20190407191305543.png" width="670" height="425"><br>上图表示60岁以上老人心脏病高发，这个和现有理论相符。</p><p>接下来看一下 <strong>2D Partial Dependence Plots</strong>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">inter = pdp.pdp_interact(model=model, </span><br><span class="line"> dataset=test_x, </span><br><span class="line"> model_features=total_features, </span><br><span class="line"> features=[&apos;oldpeak&apos;, &apos;age&apos;])</span><br><span class="line"></span><br><span class="line">pdp.pdp_interact_plot(pdp_interact_out=inter, </span><br><span class="line">  feature_names=[&apos;oldpeak&apos;, &apos;age&apos;], </span><br><span class="line">  plot_type=&apos;contour&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>Output：<br><img src="https://img-blog.csdnimg.cn/20190407192322860.png" alt><br>这个图一开始没看到，后来仔细看了<a href="https://www.kaggle.com/dansbecker/partial-plots" target="_blank" rel="noopener">Partial Dependence Plots</a> 的说明文档才搞明白。图中颜色从浅到深表示患心脏病概率降低，以最深的那个紫色为例，oldpeak &gt; 3.0 &amp;&amp; 45 &lt; age &lt; 65 时，患病概率最低，图中黄色部分表示，oldpeak &lt; 0.25 &amp;&amp;  ( age &lt; 45 || age &gt; 65 ) 时，患病概率最高。</p><h2 id="7-后记"><a href="#7-后记" class="headerlink" title="7. 后记"></a>7. 后记</h2><p>实际上本项目的数据是非常小的，其结果的可靠性也是值得怀疑的。但是通过这个项目，去经历机器学习项目的完整过程，却能学到很多东西。重要的是过程，更重要的是举一反三。该项目还引入了2个很有趣的Feature Importance Analysis的方法，对于我来说是新知识，也算是却到了。</p><p>这一篇到这里结束了，期待下一篇。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;记得有一次去面试，那个公司的HR聊天说，她感觉程序员面试那是面真功夫，会就会，不会装也没用。从这里想开来，还真是，码农学再多理论，终究是要去码砖的。我呢就是原来机器学习和深度学习的理论学的多，实践反而少，所以感觉有时候做事情就慢了些。现在趁着还有些闲工夫，就找一些项目做做，由简单到复杂，慢慢来吧。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://jkknotes.com/categories/Machine-Learning/"/>
    
      <category term="项目实战" scheme="http://jkknotes.com/categories/Machine-Learning/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="Machine Learning" scheme="http://jkknotes.com/tags/Machine-Learning/"/>
    
      <category term="Random Forest" scheme="http://jkknotes.com/tags/Random-Forest/"/>
    
      <category term="AI医疗" scheme="http://jkknotes.com/tags/AI%E5%8C%BB%E7%96%97/"/>
    
  </entry>
  
</feed>
