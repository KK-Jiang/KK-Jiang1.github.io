<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>KK&#39;s Notes</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://jkknotes.com/"/>
  <updated>2019-04-28T02:38:19.035Z</updated>
  <id>http://jkknotes.com/</id>
  
  <author>
    <name>KK.J</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hello! life（一）—— 理工男的觉悟</title>
    <link href="http://jkknotes.com/Life/Hello-life/Hello-life%EF%BC%88%E4%B8%80%EF%BC%89.html"/>
    <id>http://jkknotes.com/Life/Hello-life/Hello-life（一）.html</id>
    <published>2019-04-09T07:06:45.000Z</published>
    <updated>2019-04-28T02:38:19.035Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script>    <div id="aplayer-belBIiQa" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="760058" data-server="netase" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#FF4081"></div><blockquote><p><strong>取这么个文艺的题名，怕是写不出什么文艺的话。毕竟，真的已经变成一个理工男和直男的典型。有时候还真是羡慕自己的小时候，也就是小学初中的时候，那时候自己还写过小说，写过诗集，读了一首诗，一篇文章，常常能有感而发。再看看现在的自己，嫌弃的不行，写这篇文章的时候，竟然还想列一二三四找论据，我想写的不是一篇随笔吗，文章结构，行文走句都很随便的那种。</strong><br><a id="more"></a></p></blockquote><p>本文同步发在我的<a href="https://blog.csdn.net/iizhuzhu/article/details/89139864" target="_blank" rel="noopener">CSDN Blog</a>，欢迎各位看官大佬关注指教。</p><p>这几日闲的无聊，跑去图书馆找了个阳光充足的位置，就准备找几本闲书看看。在各个文学书架间徘徊，左翻翻右翻翻，最后敲定了几本书。翻开书，沐浴着阳光，那感觉确实不错，那种感觉还是刚上大一的时候感受过。这感觉只不过仅仅存在了几分钟罢了，竟然一篇文章也看不下去，那些散文，竟不知所云，对的，就是看不懂了，瞬间觉得自己上学这么多年，还是个粗人。</p><p>由此，不由得开始反思自己。</p><p>我开始回想自己的生活，总结着为什么。我想了下，原因至少有二：</p><p>其一，本科阶段懵懵懂懂，浪费了时间，而研究生阶段似乎懂事了，给自己压力很大，特别是实习那段时间，工作压力也不小，每天都挺累，回来躺床上只想刷微博，看Youtube，让各种碎片化的没有任何价值的新闻冲击着我的大脑，那种感觉却是会上瘾啊，而真正有营养的东西却无一席之地。久了，就习惯了快消文化占据了自己所有的碎片时间和业余生活。</p><p>我呢不过是社会发展大潮中一滴再普通不过的水滴。在这极其喧嚣和浮躁的世界里，举目四望，芸芸众生不是拿着手机就是举着手机，或是获取各种碎片化的信息，让各种情绪冲击着大脑，抑或是开着黑享受游戏的快感，间或是有几个认真的，拿着kindle，却时不时拿起手机瞟几眼。</p><p>也许是我们越来越宅了吧，圈子越来越小，好友难觅，也许是我们越来越空虚孤独，也许是压力太大或者过度放纵，也许是生活节奏太快了吧，也许是这些快消文化就是冲着我们的痛点设计的吧，让我们难以抵挡，就像是鸦片一样，让我们逐渐走向平凡，而对有营养的知识越来越难以接收。其实呢，快消文化也只是这个社会“快”的冰山一角，所有的事情都讲究快，急功近利。不说了，再说我的“愤青”怕是一发不可收拾。</p><p>其二，就吐槽吐槽学校吧，CUP真是一个极致理工的地方，极致理工男、直男癌的高产区。在这里我没有感受到自由艺术和人文思想的哪怕一点波澜。各种文艺晚会必不可少的是类似于“我为祖国献石油”式的节目。</p><p>哎，直男癌的吐槽就是这么直接。</p><p>CUP怕是中国高校中的一个极端吧，中国的传统文化向来是重文轻理的，但是中国的高等教育呢又走向重理轻文，很多理工类专业都没有设置文化通识类课程。我想这一定是不对的，这违反自然规律，因为自然规律讲究平衡。我不知道这是不是建国后没能产生大家伟人的原因之一，但是很多伟大的科学家都是博古通今的，写东西旁征博引，古诗词信手拈来，甚至能从中找出科研的灵感。再者，我觉得文化素养，人文情怀深厚的人更容易保持自我，坚持初心，不受外界影响。众所周知，高校早已经不是什么干净之地，衣冠叫兽层出不穷，出来的也只是冰山一角。扯远一点，是不是能扯上上面这个原因呢。</p><p>再随便写点。我了解的跟直男癌恋爱是很辛苦的，所以看看正面的榜样，各位看官可以学习一下。有一个学弟保研去了北大，女朋友也是北大才女，于是他收到这样的情书：</p><p><img src="http://pqngcgeuq.bkt.clouddn.com/WechatIMG53_meitu_1.jpg#pic_center =720x690" alt></p><p>看到这样的浪漫与才情，我自然是嫉妒又羡慕。受到这一万点暴击，我想我该做点什么，虽然不能写出如此有才情的情诗，但是至少治治直男癌，不然怕是真的没救了。</p>]]></content>
    
    <summary type="html">
    
      &lt;div id=&quot;aplayer-belBIiQa&quot; class=&quot;aplayer aplayer-tag-marker meting-tag-marker&quot; data-id=&quot;760058&quot; data-server=&quot;netase&quot; data-type=&quot;song&quot; data-mode=&quot;circulation&quot; data-autoplay=&quot;false&quot; data-mutex=&quot;true&quot; data-listmaxheight=&quot;340px&quot; data-preload=&quot;auto&quot; data-theme=&quot;#FF4081&quot;&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;取这么个文艺的题名，怕是写不出什么文艺的话。毕竟，真的已经变成一个理工男和直男的典型。有时候还真是羡慕自己的小时候，也就是小学初中的时候，那时候自己还写过小说，写过诗集，读了一首诗，一篇文章，常常能有感而发。再看看现在的自己，嫌弃的不行，写这篇文章的时候，竟然还想列一二三四找论据，我想写的不是一篇随笔吗，文章结构，行文走句都很随便的那种。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Life" scheme="http://jkknotes.com/categories/Life/"/>
    
      <category term="Hello! life" scheme="http://jkknotes.com/categories/Life/Hello-life/"/>
    
    
      <category term="生活" scheme="http://jkknotes.com/tags/%E7%94%9F%E6%B4%BB/"/>
    
      <category term="情感" scheme="http://jkknotes.com/tags/%E6%83%85%E6%84%9F/"/>
    
      <category term="随笔" scheme="http://jkknotes.com/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>kaggle实战——What Causes Heart Disease?</title>
    <link href="http://jkknotes.com/Machine-Learning/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/kaggle%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94What-Causes-Heart-Disease.html"/>
    <id>http://jkknotes.com/Machine-Learning/项目实战/kaggle实战——What-Causes-Heart-Disease.html</id>
    <published>2019-04-07T12:29:29.000Z</published>
    <updated>2019-04-28T02:55:30.617Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><strong>记得有一次去面试，那个公司的HR聊天说，她感觉程序员面试那是面真功夫，会就会，不会装也没用。从这里想开来，还真是，码农学再多理论，终究是要去码砖的。我呢就是原来机器学习和深度学习的理论学的多，实践反而少，所以感觉有时候做事情就慢了些。现在趁着还有些闲工夫，就找一些项目做做，由简单到复杂，慢慢来吧。</strong><br><a id="more"></a></p></blockquote><p>本文同步发在我的<a href="https://blog.csdn.net/iizhuzhu/article/details/89067386" target="_blank" rel="noopener">CSDN Blog</a>，接下来CSDN 和 KK’s Notes 同步更新，各位看官大佬多多指教。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>这个项目来自于<a href="https://www.kaggle.com/tentotheminus9/what-causes-heart-disease-explaining-the-model/notebook" target="_blank" rel="noopener">kaggle</a>。项目主要是利用患者的个人信息和检查数据，利用机器学习方法来诊断该患者收否患疾病，并且尝试对识别结果作出解释。这个项目虽然简单但将机器学习的全流程和常用预处理和分析方法都涉及到了，我做完一遍还是有很多收获。以下操作皆在 <strong>Jubyter notebook</strong> 下以 <strong>Python</strong> 进行的。</p><p>主要使用的技术：</p><ul><li>Random Forest</li><li>Feature Importance Analysis: <strong>Permutation importance</strong></li><li>Feature Importance Analysis: <strong>Partial Dependence Plots</strong></li></ul><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>Data from：<a href="https://www.kaggle.com/ronitf/heart-disease-uci/downloads/heart.csv/" target="_blank" rel="noopener">https://www.kaggle.com/ronitf/heart-disease-uci/downloads/heart.csv/</a><br>About Data：下载好数据之后直接打开看一看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = pd.read_csv(<span class="string">'data/heart.csv'</span>)</span><br><span class="line">data.info()</span><br></pre></td></tr></table></figure><p>Output:<br><img src="http://pqngcgeuq.bkt.clouddn.com/b2.png" width="342" height="334"><br>可以看到总共有303条数据以及13个特征和1个标签，数据没有缺失项。接下看下前十个数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure></p><p>Output:<br><img src="http://pqngcgeuq.bkt.clouddn.com/b1.png" width="621" height="308"><br>这13个特征的含义分别是：</p><blockquote><p>age: 年龄<br>sex：该人的性别（1=男性，0=女性）<br>cp：胸痛经历（值1：典型心绞痛，值2：非典型心绞痛，值3：非心绞痛，值4：无症状）<br>trestbps：该人的静息血压（入院时为mm Hg）<br>chol：人体胆固醇测量单位为mg/dl<br>fbs：该人的空腹血糖（&gt; 120mg/dl，1=true; 0= f=alse）<br>restecg：静息心电图测量（0=正常，1=有ST-T波异常，2=按Estes标准显示可能或明确的左心室肥厚）<br>thalach：达到了该人的最大心率<br>exang：运动诱发心绞痛（1=是; 0=否）<br>oldpeak：运动相对于休息引起的ST段压低（’ST’与ECG图上的位置有关）<br>slope：峰值运动ST段的斜率（值1：上升，值2：平坦，值3：下降）<br>ca：主要血管数量（0-3）<br>thal：称为地中海贫血的血液疾病（1=正常; 2=固定缺陷; 3=可逆缺陷）<br>target：心脏病（0=不，1=是）</p></blockquote><p>为了更好的理解数据，我们应该提前查一下每个特征的含义，以及医学上该特征和心脏病的关系。具体这里不再赘述。</p><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>这里为了方便后续做心脏病诊断中影响因素分析即Feature Importance Analysis（还是觉得用英文更能表达意思），将部分数值型特征进行转换：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">data.loc[data.sex == <span class="number">1</span>, <span class="string">'sex'</span>] = <span class="string">'male'</span></span><br><span class="line">data.loc[data[<span class="string">'sex'</span>] == <span class="number">0</span>, <span class="string">'sex'</span>] = <span class="string">'female'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'cp'</span>] == <span class="number">1</span>, <span class="string">'cp'</span>] = <span class="string">'typical'</span></span><br><span class="line">data.loc[data[<span class="string">'cp'</span>] == <span class="number">2</span>, <span class="string">'cp'</span>] = <span class="string">'atypical'</span></span><br><span class="line">data.loc[data[<span class="string">'cp'</span>] == <span class="number">3</span>, <span class="string">'cp'</span>] = <span class="string">'no_pain'</span></span><br><span class="line">data.loc[data[<span class="string">'cp'</span>] == <span class="number">4</span>, <span class="string">'cp'</span>] = <span class="string">'no_feel'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'fbs'</span>] == <span class="number">1</span>, <span class="string">'fbs'</span>] = <span class="string">'higher than 120 mg/dl'</span></span><br><span class="line">data.loc[data[<span class="string">'fbs'</span>] == <span class="number">0</span>, <span class="string">'fbs'</span>] = <span class="string">'lower than 120 mg/dl'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'restecg'</span>] == <span class="number">0</span>, <span class="string">'restecg'</span>] = <span class="string">'normal'</span></span><br><span class="line">data.loc[data[<span class="string">'restecg'</span>] == <span class="number">1</span>, <span class="string">'restecg'</span>] = <span class="string">'ST-T wave abnormality'</span></span><br><span class="line">data.loc[data[<span class="string">'restecg'</span>] == <span class="number">2</span>, <span class="string">'restecg'</span>] = <span class="string">'left ventricular hypertrophy'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'exang'</span>] == <span class="number">1</span>, <span class="string">'exang'</span>] = <span class="string">'true'</span></span><br><span class="line">data.loc[data[<span class="string">'exang'</span>] == <span class="number">0</span>, <span class="string">'exang'</span>] = <span class="string">'false'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'slope'</span>] == <span class="number">1</span>, <span class="string">'slope'</span>] = <span class="string">'up'</span></span><br><span class="line">data.loc[data[<span class="string">'slope'</span>] == <span class="number">2</span>, <span class="string">'slope'</span>] = <span class="string">'flat'</span></span><br><span class="line">data.loc[data[<span class="string">'slope'</span>] == <span class="number">3</span>, <span class="string">'slope'</span>] = <span class="string">'down'</span></span><br><span class="line"></span><br><span class="line">data.loc[data[<span class="string">'thal'</span>] == <span class="number">1</span>, <span class="string">'thal'</span>] = <span class="string">'normal'</span></span><br><span class="line">data.loc[data[<span class="string">'thal'</span>] == <span class="number">2</span>, <span class="string">'thal'</span>] = <span class="string">'fixed defect'</span></span><br><span class="line">data.loc[data[<span class="string">'thal'</span>] == <span class="number">3</span>, <span class="string">'thal'</span>] = <span class="string">'reversable defect'</span></span><br></pre></td></tr></table></figure></p><p>检查下数据情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.describe(include=[np.object])</span><br></pre></td></tr></table></figure><p>Output:<br><img src="http://pqngcgeuq.bkt.clouddn.com/b4.png" width="560" height="147"><br>可以看到特征thal有4个值，而我们在转换时只转换了3个。实际上thal存在2个缺失值用0补齐的。为了防止数据类型错误，这里做一下类型转换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'thal'</span>] = data[<span class="string">'thal'</span>].astype(<span class="string">'object'</span>)</span><br></pre></td></tr></table></figure><p>再看下数据：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.head()</span><br></pre></td></tr></table></figure></p><p>Output:<br><img src="http://pqngcgeuq.bkt.clouddn.com/b3.png" width="889" height="174"><br>模型的训练肯定需要数值型特征。这里对特征进行Onehot编码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = pd.get_dummies(data, drop_first=<span class="literal">True</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><p>Output：<br><img src="http://pqngcgeuq.bkt.clouddn.com/b5.png" width="1011" height="186"><br>（由于我还不知道在用markdown编辑时怎么显示运行结果，这里用的是截图，只能截取一部分，还有特征没有截取出来）<br>数据预处理部分就到此为止，接下来上模型。</p><h2 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h2><p>对于 Random Forest 的原理这里就不介绍了，网上介绍的文章也很多。废话不多说，直接import package.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><p>将数据分成 train_data 和 test_data 2个集合，二者比例为8:2。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_x, test_x, train_y, test_y = train_test_split(data.drop(columns=<span class="string">'target'</span>),</span><br><span class="line">                                                    data[<span class="string">'target'</span>],</span><br><span class="line">                                                    test_size=<span class="number">0.2</span>,</span><br><span class="line">                                                    random_state=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>简单的画个图调个参。这里 Random Forest 主要的参数有基学习器决策树的最大深度（这里依据经验选5）、基学习器个数 n_estimators。这里基学习器选用CART。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">train_score = []</span><br><span class="line">test_score = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>):</span><br><span class="line">    model = RandomForestClassifier(max_depth=<span class="number">5</span>,</span><br><span class="line">                                   n_estimators=n，</span><br><span class="line">                                   criterion=<span class="string">'gini'</span>)</span><br><span class="line">    model.fit(train_x, train_y)</span><br><span class="line">    train_score.append(model.score(train_x, train_y))</span><br><span class="line">    test_score.append(model.score(test_x, test_y))</span><br></pre></td></tr></table></figure><p>训练完，把train和test上的accuracy随基学习器个数的变化画成图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x_axis = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>)]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(x_axis, train_score[:<span class="number">99</span>])</span><br><span class="line">ax.plot(x_axis, test_score[:<span class="number">99</span>], c=<span class="string">"r"</span>)</span><br><span class="line">plt.xlim([<span class="number">0</span>, <span class="number">100</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.rcParams[<span class="string">'font.size'</span>] = <span class="number">12</span></span><br><span class="line">plt.xlabel(<span class="string">'n_estimators'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'accuracy'</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>Output：<br><img src="http://pqngcgeuq.bkt.clouddn.com/b6.png" alt><br>可以看到大概是n_estimators=14的时候效果最好，train和test上的accuracy分别是0.9463，0.8361。看上去没有那么差。</p><h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2><p>训练完模型，用<a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank" rel="noopener">ROC曲线</a>来评估下模型的效果。ROC曲线事宜FPR和TPR分别为横纵轴作出的曲线，其和坐标轴围成的面积越大，说明模型效果越好。具体评判标准见下文。说一下几个概念：</p><blockquote><ul><li>TPR: 真正例率，表示所有真正为正例的样本被正确预测出来的比例，等同于Recall</li><li>FNR: 假负例率，FNR = 1 - TPR</li><li>FPR: 假正例率，表示所有负例中被预测为正例的比例。</li><li>TNR: 真负例率，TNR = 1 - FPR</li></ul></blockquote><p>好吧，我也快晕了。<br>接下来计算一下正例和负例的recall</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> auc, roc_curve</span><br><span class="line"></span><br><span class="line"><span class="comment"># 混淆矩阵</span></span><br><span class="line">confusion_m = confusion_matrix(test_y, pred_y) </span><br><span class="line"><span class="keyword">print</span> confusion_m</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[29  6]</span><br><span class="line"> [ 4 22]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total = confusion_m.sum()</span><br><span class="line">tpr = float(confusion_m[<span class="number">0</span>][<span class="number">0</span>]) / (confusion_m[<span class="number">0</span>][<span class="number">0</span>] + confusion_m[<span class="number">1</span>][<span class="number">0</span>])</span><br><span class="line">tnr = float(confusion_m[<span class="number">1</span>][<span class="number">1</span>]) / (confusion_m[<span class="number">1</span>][<span class="number">1</span>] + confusion_m[<span class="number">0</span>][<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> tpr, tnr</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.878787878788 0.785714285714</span><br></pre></td></tr></table></figure><p>Just so so!!</p><p>画ROC曲线图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pred_y = model.predict(test_x)  <span class="comment"># 预测结果</span></span><br><span class="line">pred_prob_y = model.predict_proba(test_x)[:, <span class="number">1</span>]  <span class="comment"># 为正例的概率</span></span><br><span class="line">fpr_list, tpr_list, throsholds = roc_curve(test_y, pred_prob_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(fpr_list, tpr_list)</span><br><span class="line">ax.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], transform=ax.transAxes, ls=<span class="string">"--"</span>, c=<span class="string">"r"</span>)</span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.rcParams[<span class="string">'font.size'</span>] = <span class="number">12</span></span><br><span class="line">plt.title(<span class="string">'roc curve'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'fpr'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'tpr'</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>Output:<br><img src="http://pqngcgeuq.bkt.clouddn.com/ROC.png" alt><br>前文说了，ROC曲线和坐标轴围成的面积越大，说明模型效果越好。这个面积就叫 AUC .根据AUC的值，可参考下面的规则评估模型：</p><blockquote><ul><li>0.90 - 1.00 = excellent</li><li>0.80 - 0.90 = good</li><li>0.70 - 0.80 = fair</li><li>0.60 - 0.70 = poor</li><li>0.50 - 0.60 = fail</li></ul></blockquote><p>看看我们训练模型的AUC</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">auc(fpr_list, tpr_list)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9032967032967033</span><br></pre></td></tr></table></figure><p>OK， working well！<br><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1554636629238&amp;di=b77d71e635f8ed2e7fa40c847ac1b893&amp;imgtype=0&amp;src=http://b-ssl.duitang.com/uploads/item/201703/29/20170329161728_fdSMF.thumb.224_0.gif" alt></p><h2 id="Feature-Importance-Analysis"><a href="#Feature-Importance-Analysis" class="headerlink" title="Feature Importance Analysis"></a>Feature Importance Analysis</h2><p>训练完模型，我们希望能从模型里得到点什么， 比如说哪些特征对模型结果贡献率比较大，是不是意味着这些影响因素在实际心脏病诊断中也是很重要对参考，或者说还能发现一些现有医学没有发现的发现。所有接下来我们做的是一件很有意思的事。</p><h3 id="决策树可视化"><a href="#决策树可视化" class="headerlink" title="决策树可视化"></a>决策树可视化</h3><p>如果我没记错的话， 根据决策树的原理，越先分裂的特征越重要。那么下面对决策树进行可视化，看看它到底做了什么。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出 feature_name</span></span><br><span class="line">estimator = model.estimators_[<span class="number">1</span>]</span><br><span class="line">features = [i <span class="keyword">for</span> i <span class="keyword">in</span> train_x.columns]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0 —&gt; no disease，1 —&gt; disease</span></span><br><span class="line">train_y_str = train_y.astype(<span class="string">'str'</span>)</span><br><span class="line">train_y_str[train_y_str == <span class="string">'0'</span>] = <span class="string">'no disease'</span></span><br><span class="line">train_y_str[train_y_str == <span class="string">'1'</span>] = <span class="string">'disease'</span></span><br><span class="line">train_y_str = train_y_str.values</span><br></pre></td></tr></table></figure><p>sklearn 真是个好东西，你能想到对功能他都有。下面用 sklearn 的 export_graphviz 对决策树进行可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export_graphviz(estimator, out_file=<span class="string">'tree.dot'</span>, </span><br><span class="line">                feature_names = features,</span><br><span class="line">                class_names = train_y_str,</span><br><span class="line">                rounded = <span class="literal">True</span>, proportion = <span class="literal">True</span>, </span><br><span class="line">                label=<span class="string">'root'</span>,</span><br><span class="line">                precision = <span class="number">2</span>, filled = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>生成对这个 tree.dot 文件还不能直接看，网上查了一下，把它输出来看看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pydotplus</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</span><br><span class="line">img = pydotplus.graph_from_dot_file(<span class="string">'tree.dot'</span>)</span><br><span class="line"><span class="comment">#img.write_pdf('tree.pdf') #输出成PDF</span></span><br><span class="line">Image(img.create_png())</span><br></pre></td></tr></table></figure><p>Output：<br><img src="http://pqngcgeuq.bkt.clouddn.com/tree.png" alt><br>实际上这张图就解释来决策树的生成过程。一般我们认为最先分裂的特征越重要，但是从这张图我们并不能很直观的看出特征的重要性。</p><h3 id="Permutation-importance"><a href="#Permutation-importance" class="headerlink" title="Permutation importance"></a>Permutation importance</h3><p>我们换一个工具—<a href="https://www.kaggle.com/dansbecker/permutation-importance" target="_blank" rel="noopener">Permutation importance</a>. 其原理是依次打乱test_data中其中一个特征数值的顺序，其实就是做shuffle，然后观察模型的效果，下降的多的说明这个特征对模型比较重要。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> eli5</span><br><span class="line"><span class="keyword">from</span> eli5.sklearn <span class="keyword">import</span> PermutationImportance</span><br><span class="line"></span><br><span class="line">perm = PermutationImportance(model, random_state=<span class="number">20</span>).fit(test_x, test_y)</span><br><span class="line">eli5.show_weights(perm, feature_names=test_x.columns.tolist())</span><br></pre></td></tr></table></figure><p>Output：<br><img src="http://pqngcgeuq.bkt.clouddn.com/feature_w.png" width="323" height="290"><br>一目了然，一切尽在不言中。还是说俩句吧，绿色越深表示正相关越强，红色越深表示负相关越强。<br>实际上我发现改变 PermutationImportance 的参数 random_state 的值结果变化挺大的，不过还是有几个特征位次变化不大，结果还是具有参考意义。</p><h3 id="Partial-Dependence-Plots"><a href="#Partial-Dependence-Plots" class="headerlink" title="Partial Dependence Plots"></a>Partial Dependence Plots</h3><p>我们试试另一个工具—<a href="https://www.kaggle.com/dansbecker/partial-plots" target="_blank" rel="noopener">Partial Dependence Plots</a>. 其原理和 Permutation importance 有点类似，当它判断一个特征对模型的影响时，对于所有样本，将该特征依次取该特征的所有取值，观察模型结果的变化。先画图，再根据图解释一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pdpbox <span class="keyword">import</span> pdp, info_plots</span><br><span class="line"></span><br><span class="line">total_features = train_x.columns.values.tolist()</span><br><span class="line">feature_name = <span class="string">'oldpeak'</span></span><br><span class="line">pdp_dist = pdp.pdp_isolate(model=model, dataset=test_x, model_features=total_features, feature=feature_name)</span><br><span class="line"></span><br><span class="line">pdp.pdp_plot(pdp_dist, feature_name)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>Output：<br><img src="http://pqngcgeuq.bkt.clouddn.com/b7.png" width="670" height="425"><br>上图的纵坐标是模型相对于base model 的变化，横坐标是该特征的所有取值，实线表示相对于base model 的变化的平均值，蓝色阴影表示置信度。oldpeak表示运动相对于休息引起的ST段压低，可以看到其取值越大，患心脏病的可能性越低。不知道这个结果可不可信，我觉得需要医学知识作支撑。</p><p>又试了几个特征：</p><p><strong>Sex：</strong><br><img src="http://pqngcgeuq.bkt.clouddn.com/b8.png" width="670" height="425"><br>上图说明男性比女性患心脏病的概率要低些，网上查了一下，还真是这样。</p><p><strong>Age：</strong><br><img src="http://pqngcgeuq.bkt.clouddn.com/b9.png" width="670" height="425"><br>上图表示60岁以上老人心脏病高发，这个和现有理论相符。</p><p>接下来看一下 <strong>2D Partial Dependence Plots</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">inter = pdp.pdp_interact(model=model, </span><br><span class="line">         dataset=test_x, </span><br><span class="line"> model_features=total_features, </span><br><span class="line"> features=[<span class="string">'oldpeak'</span>, <span class="string">'age'</span>])</span><br><span class="line"></span><br><span class="line">pdp.pdp_interact_plot(pdp_interact_out=inter, </span><br><span class="line">      feature_names=[<span class="string">'oldpeak'</span>, <span class="string">'age'</span>], </span><br><span class="line">      plot_type=<span class="string">'contour'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>Output：<br><img src="http://pqngcgeuq.bkt.clouddn.com/b10.png" alt><br>这个图一开始没看到，后来仔细看了<a href="https://www.kaggle.com/dansbecker/partial-plots" target="_blank" rel="noopener">Partial Dependence Plots</a> 的说明文档才搞明白。图中颜色从浅到深表示患心脏病概率降低，以最深的那个紫色为例，oldpeak &gt; 3.0 &amp;&amp; 45 &lt; age &lt; 65 时，患病概率最低，图中黄色部分表示，oldpeak &lt; 0.25 &amp;&amp;  ( age &lt; 45 || age &gt; 65 ) 时，患病概率最高。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>实际上本项目的数据是非常小的，其结果的可靠性也是值得怀疑的。但是通过这个项目，去经历机器学习项目的完整过程，却能学到很多东西。重要的是过程，更重要的是举一反三。该项目还引入了2个很有趣的Feature Importance Analysis的方法，对于我来说是新知识，也算是学到了。</p><p>这一篇到这里结束了，期待下一篇。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;记得有一次去面试，那个公司的HR聊天说，她感觉程序员面试那是面真功夫，会就会，不会装也没用。从这里想开来，还真是，码农学再多理论，终究是要去码砖的。我呢就是原来机器学习和深度学习的理论学的多，实践反而少，所以感觉有时候做事情就慢了些。现在趁着还有些闲工夫，就找一些项目做做，由简单到复杂，慢慢来吧。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://jkknotes.com/categories/Machine-Learning/"/>
    
      <category term="项目实战" scheme="http://jkknotes.com/categories/Machine-Learning/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="Machine Learning" scheme="http://jkknotes.com/tags/Machine-Learning/"/>
    
      <category term="Random Forest" scheme="http://jkknotes.com/tags/Random-Forest/"/>
    
      <category term="AI医疗" scheme="http://jkknotes.com/tags/AI%E5%8C%BB%E7%96%97/"/>
    
  </entry>
  
</feed>
