{"meta":{"title":"KK's Notes","subtitle":null,"description":"Write and do better.","author":"KK.J","url":"http://jkknotes.com","root":"/"},"pages":[{"title":"categories","date":"2019-04-04T06:05:37.000Z","updated":"2019-04-04T07:37:09.070Z","comments":true,"path":"categories/index.html","permalink":"http://jkknotes.com/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2019-04-04T07:43:55.000Z","updated":"2019-04-06T10:48:12.938Z","comments":true,"path":"about/index.html","permalink":"http://jkknotes.com/about/index.html","excerpt":"","text":"我々が岩壁の花を美しく思うのは我々が岩壁に足を止めてしまうからだ恐れ悚れ无き その花のように空へと踏み出せずにいるからだ我们之所以觉得悬崖上的花朵美丽那是因为我们会在悬崖停下脚步而不是像那些毫不畏惧的花朵般能向天空踏出一步—— BLEACH.12"},{"title":"tags","date":"2019-04-04T07:43:15.000Z","updated":"2019-04-04T07:46:36.344Z","comments":true,"path":"tags/index.html","permalink":"http://jkknotes.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Hello World","slug":"hello-world","date":"2019-04-07T12:35:19.810Z","updated":"2019-04-07T12:35:19.810Z","comments":true,"path":"Life/hello-world.html","link":"","permalink":"http://jkknotes.com/Life/hello-world.html","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"Life","slug":"Life","permalink":"http://jkknotes.com/categories/Life/"}],"tags":[{"name":"study","slug":"study","permalink":"http://jkknotes.com/tags/study/"},{"name":"tool","slug":"tool","permalink":"http://jkknotes.com/tags/tool/"}]},{"title":"kaggle实战——What Causes Heart Disease?","slug":"kaggle实战——What-Causes-Heart-Disease","date":"2019-04-07T12:29:29.000Z","updated":"2019-04-07T15:08:55.026Z","comments":true,"path":"Machine-Learning/项目实战/kaggle实战——What-Causes-Heart-Disease.html","link":"","permalink":"http://jkknotes.com/Machine-Learning/项目实战/kaggle实战——What-Causes-Heart-Disease.html","excerpt":"记得有一次去面试，那个公司的HR聊天说，她感觉程序员面试那是面真功夫，会就会，不会装也没用。从这里想开来，还真是，码农学再多理论，终究是要去码砖的。我呢就是原来机器学习和深度学习的理论学的多，实践反而少，所以感觉有时候做事情就慢了些。现在趁着还有些闲工夫，就找一些项目做做，由简单到复杂，慢慢来吧。","text":"记得有一次去面试，那个公司的HR聊天说，她感觉程序员面试那是面真功夫，会就会，不会装也没用。从这里想开来，还真是，码农学再多理论，终究是要去码砖的。我呢就是原来机器学习和深度学习的理论学的多，实践反而少，所以感觉有时候做事情就慢了些。现在趁着还有些闲工夫，就找一些项目做做，由简单到复杂，慢慢来吧。 本文同步发在我的CSDN Blog，接下来CSDN 和 KK’s Notes 同步更新，各位看官大佬多多指教。 Introduction这个项目来自于kaggle。项目主要是利用患者的个人信息和检查数据，利用机器学习方法来诊断该患者收否患疾病，并且尝试对识别结果作出解释。这个项目虽然简单但将机器学习的全流程和常用预处理和分析方法都涉及到了，我做完一遍还是有很多收获。以下操作皆在 jubyter notebook 下以 python 进行的。 主要使用的技术： Random Forest Feature Importance Analysis: Permutation importance Feature Importance Analysis: Partial Dependence Plots DataData from：https://www.kaggle.com/ronitf/heart-disease-uci/downloads/heart.csv/About Data：下载好数据之后直接打开看一看。 1234import pandas as pdimport numpy as npdata = pd.read_csv(&apos;data/heart.csv&apos;)data.info() Output:可以看到总共有303条数据以及13个特征和1个标签，数据没有缺失项。接下看下前十个数据。1data.head(10) Output:这13个特征的含义分别是： age: 年龄sex：该人的性别（1=男性，0=女性）cp：胸痛经历（值1：典型心绞痛，值2：非典型心绞痛，值3：非心绞痛，值4：无症状）trestbps：该人的静息血压（入院时为mm Hg）chol：人体胆固醇测量单位为mg/dlfbs：该人的空腹血糖（&gt; 120mg/dl，1=true; 0= f=alse）restecg：静息心电图测量（0=正常，1=有ST-T波异常，2=按Estes标准显示可能或明确的左心室肥厚）thalach：达到了该人的最大心率exang：运动诱发心绞痛（1=是; 0=否）oldpeak：运动相对于休息引起的ST段压低（’ST’与ECG图上的位置有关）slope：峰值运动ST段的斜率（值1：上升，值2：平坦，值3：下降）ca：主要血管数量（0-3）thal：称为地中海贫血的血液疾病（1=正常; 2=固定缺陷; 3=可逆缺陷）target：心脏病（0=不，1=是） 为了更好的理解数据，我们应该提前查一下每个特征的含义，以及医学上该特征和心脏病的关系。具体这里不再赘述。 数据预处理这里为了方便后续做心脏病诊断中影响因素分析即Feature Importance Analysis（还是觉得用英文更能表达意思），将部分数值型特征进行转换：12345678910111213141516171819202122232425data.loc[data.sex == 1, &apos;sex&apos;] = &apos;male&apos;data.loc[data[&apos;sex&apos;] == 0, &apos;sex&apos;] = &apos;female&apos;data.loc[data[&apos;cp&apos;] == 1, &apos;cp&apos;] = &apos;typical&apos;data.loc[data[&apos;cp&apos;] == 2, &apos;cp&apos;] = &apos;atypical&apos;data.loc[data[&apos;cp&apos;] == 3, &apos;cp&apos;] = &apos;no_pain&apos;data.loc[data[&apos;cp&apos;] == 4, &apos;cp&apos;] = &apos;no_feel&apos;data.loc[data[&apos;fbs&apos;] == 1, &apos;fbs&apos;] = &apos;higher than 120 mg/dl&apos;data.loc[data[&apos;fbs&apos;] == 0, &apos;fbs&apos;] = &apos;lower than 120 mg/dl&apos;data.loc[data[&apos;restecg&apos;] == 0, &apos;restecg&apos;] = &apos;normal&apos;data.loc[data[&apos;restecg&apos;] == 1, &apos;restecg&apos;] = &apos;ST-T wave abnormality&apos;data.loc[data[&apos;restecg&apos;] == 2, &apos;restecg&apos;] = &apos;left ventricular hypertrophy&apos;data.loc[data[&apos;exang&apos;] == 1, &apos;exang&apos;] = &apos;true&apos;data.loc[data[&apos;exang&apos;] == 0, &apos;exang&apos;] = &apos;false&apos;data.loc[data[&apos;slope&apos;] == 1, &apos;slope&apos;] = &apos;up&apos;data.loc[data[&apos;slope&apos;] == 2, &apos;slope&apos;] = &apos;flat&apos;data.loc[data[&apos;slope&apos;] == 3, &apos;slope&apos;] = &apos;down&apos;data.loc[data[&apos;thal&apos;] == 1, &apos;thal&apos;] = &apos;normal&apos;data.loc[data[&apos;thal&apos;] == 2, &apos;thal&apos;] = &apos;fixed defect&apos;data.loc[data[&apos;thal&apos;] == 3, &apos;thal&apos;] = &apos;reversable defect&apos; 检查下数据情况： 1data.describe(include=[np.object]) Output:可以看到特征thal有4个值，而我们在转换时只转换了3个。实际上thal存在2个缺失值用0补齐的。为了防止数据类型错误，这里做一下类型转换。 1data[&apos;thal&apos;] = data[&apos;thal&apos;].astype(&apos;object&apos;) 再看下数据：1data.head() Output:模型的训练肯定需要数值型特征。这里对特征进行Onehot编码。 12data = pd.get_dummies(data, drop_first=True)data.head() Output：（由于我还不知道在用markdown编辑时怎么显示运行结果，这里用的是截图，只能截取一部分，还有特征没有截取出来）数据预处理部分就到此为止，接下来上模型。 Random Forest对于 Random Forest 的原理这里就不介绍了，网上介绍的文章也很多。废话不多说，直接import package. 1234from sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestClassifierimport matplotlib.pyplot as plt 将数据分成 train_data 和 test_data 2个集合，二者比例为8:2。 1234train_x, test_x, train_y, test_y = train_test_split(data.drop(columns=&apos;target&apos;), data[&apos;target&apos;], test_size=0.2, random_state=10) 简单的画个图调个参。这里 Random Forest 主要的参数有基学习器决策树的最大深度（这里依据经验选5）、基学习器个数 n_estimators。这里基学习器选用CART。 12345678910train_score = []test_score = []for n in range(1, 100): model = RandomForestClassifier(max_depth=5, n_estimators=n， criterion=&apos;gini&apos;) model.fit(train_x, train_y) train_score.append(model.score(train_x, train_y)) test_score.append(model.score(test_x, test_y)) 训练完，把train和test上的accuracy随基学习器个数的变化画成图。 1234567891011x_axis = [i for i in range(1, 100)]fig, ax = plt.subplots()ax.plot(x_axis, train_score[:99])ax.plot(x_axis, test_score[:99], c=&quot;r&quot;)plt.xlim([0, 100])plt.ylim([0.0, 1.0])plt.rcParams[&apos;font.size&apos;] = 12plt.xlabel(&apos;n_estimators&apos;)plt.ylabel(&apos;accuracy&apos;)plt.grid(True) Output：可以看到大概是n_estimators=14的时候效果最好，train和test上的accuracy分别是0.9463，0.8361。看上去没有那么差。 模型评估训练完模型，用ROC曲线来评估下模型的效果。ROC曲线事宜FPR和TPR分别为横纵轴作出的曲线，其和坐标轴围成的面积越大，说明模型效果越好。具体评判标准见下文。说一下几个概念： TPR: 真正例率，表示所有真正为正例的样本被正确预测出来的比例，等同于Recall FNR: 假负例率，FNR = 1 - TPR FPR: 假正例率，表示所有负例中被预测为正例的比例。 TNR: 真负例率，TNR = 1 - FPR 好吧，我也快晕了。接下来计算一下正例和负例的recall 123456from sklearn.metrics import confusion_matrixfrom sklearn.metrics import auc, roc_curve# 混淆矩阵confusion_m = confusion_matrix(test_y, pred_y) print confusion_m Output: 12[[29 6] [ 4 22]] 1234total = confusion_m.sum()tpr = float(confusion_m[0][0]) / (confusion_m[0][0] + confusion_m[1][0])tnr = float(confusion_m[1][1]) / (confusion_m[1][1] + confusion_m[0][1])print tpr, tnr Output: 10.878787878788 0.785714285714 Just so so!! 画ROC曲线图： 123456789101112131415pred_y = model.predict(test_x) # 预测结果pred_prob_y = model.predict_proba(test_x)[:, 1] # 为正例的概率fpr_list, tpr_list, throsholds = roc_curve(test_y, pred_prob_y)# 画图fig, ax = plt.subplots()ax.plot(fpr_list, tpr_list)ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=&quot;--&quot;, c=&quot;r&quot;)plt.xlim([0.0, 1.0])plt.ylim([0.0, 1.0])plt.rcParams[&apos;font.size&apos;] = 12plt.title(&apos;roc curve&apos;)plt.xlabel(&apos;fpr&apos;)plt.ylabel(&apos;tpr&apos;)plt.grid(True) Output:前文说了，ROC曲线和坐标轴围成的面积越大，说明模型效果越好。这个面积就叫 AUC .根据AUC的值，可参考下面的规则评估模型： 0.90 - 1.00 = excellent 0.80 - 0.90 = good 0.70 - 0.80 = fair 0.60 - 0.70 = poor 0.50 - 0.60 = fail 看看我们训练模型的AUC 1auc(fpr_list, tpr_list) Output: 10.9032967032967033 OK， working well！ Feature Importance Analysis训练完模型，我们希望能从模型里得到点什么， 比如说哪些特征对模型结果贡献率比较大，是不是意味着这些影响因素在实际心脏病诊断中也是很重要对参考，或者说还能发现一些现有医学没有发现的发现。所有接下来我们做的是一件很有意思的事。 决策树可视化如果我没记错的话， 根据决策树的原理，越先分裂的特征越重要。那么下面对决策树进行可视化，看看它到底做了什么。 1234567891011from sklearn.tree import export_graphviz# 输出 feature_nameestimator = model.estimators_[1]features = [i for i in train_x.columns]# 0 —&gt; no disease，1 —&gt; diseasetrain_y_str = train_y.astype(&apos;str&apos;)train_y_str[train_y_str == &apos;0&apos;] = &apos;no disease&apos;train_y_str[train_y_str == &apos;1&apos;] = &apos;disease&apos;train_y_str = train_y_str.values sklearn 真是个好东西，你能想到对功能他都有。下面用 sklearn 的 export_graphviz 对决策树进行可视化。 123456export_graphviz(estimator, out_file=&apos;tree.dot&apos;, feature_names = features, class_names = train_y_str, rounded = True, proportion = True, label=&apos;root&apos;, precision = 2, filled = True) 生成对这个 tree.dot 文件还不能直接看，网上查了一下，把它输出来看看。 12345import pydotplusfrom IPython.display import Imageimg = pydotplus.graph_from_dot_file(&apos;tree.dot&apos;)#img.write_pdf(&apos;tree.pdf&apos;) #输出成PDFImage(img.create_png()) Output：实际上这张图就解释来决策树的生成过程。一般我们认为最先分裂的特征越重要，但是从这张图我们并不能很直观的看出特征的重要性。 Permutation importance我们换一个工具—Permutation importance. 其原理是依次打乱test_data中其中一个特征数值的顺序，其实就是做shuffle，然后观察模型的效果，下降的多的说明这个特征对模型比较重要。 12345import eli5from eli5.sklearn import PermutationImportanceperm = PermutationImportance(model, random_state=20).fit(test_x, test_y)eli5.show_weights(perm, feature_names=test_x.columns.tolist()) Output：一目了然，一切尽在不言中。还是说俩句吧，绿色越深表示正相关越强，红色越深表示负相关越强。实际上我发现改变 PermutationImportance 的参数 random_state 的值结果变化挺大的，不过还是有几个特征位次变化不大，结果还是具有参考意义。 Partial Dependence Plots我们试试另一个工具—Partial Dependence Plots. 其原理和 Permutation importance 有点类似，当它判断一个特征对模型的影响时，对于所有样本，将该特征依次取该特征的所有取值，观察模型结果的变化。先画图，再根据图解释一下。 12345678from pdpbox import pdp, info_plotstotal_features = train_x.columns.values.tolist()feature_name = &apos;oldpeak&apos;pdp_dist = pdp.pdp_isolate(model=model, dataset=test_x, model_features=total_features, feature=feature_name)pdp.pdp_plot(pdp_dist, feature_name)plt.show() Output：上图的纵坐标是模型相对于base model 的变化，横坐标是该特征的所有取值，实线表示相对于base model 的变化的平均值，蓝色阴影表示置信度。oldpeak表示运动相对于休息引起的ST段压低，可以看到其取值越大，患心脏病的可能性越低。不知道这个结果可不可信，我觉得需要医学知识作支撑。 又试了几个特征： Sex：上图说明男性比女性患心脏病的概率要低些，网上查了一下，还真是这样。 Age：上图表示60岁以上老人心脏病高发，这个和现有理论相符。 接下来看一下 2D Partial Dependence Plots. 123456789inter = pdp.pdp_interact(model=model, dataset=test_x, model_features=total_features, features=[&apos;oldpeak&apos;, &apos;age&apos;])pdp.pdp_interact_plot(pdp_interact_out=inter, feature_names=[&apos;oldpeak&apos;, &apos;age&apos;], plot_type=&apos;contour&apos;)plt.show() Output：这个图一开始没看到，后来仔细看了Partial Dependence Plots 的说明文档才搞明白。图中颜色从浅到深表示患心脏病概率降低，以最深的那个紫色为例，oldpeak &gt; 3.0 &amp;&amp; 45 &lt; age &lt; 65 时，患病概率最低，图中黄色部分表示，oldpeak &lt; 0.25 &amp;&amp; ( age &lt; 45 || age &gt; 65 ) 时，患病概率最高。 后记实际上本项目的数据是非常小的，其结果的可靠性也是值得怀疑的。但是通过这个项目，去经历机器学习项目的完整过程，却能学到很多东西。重要的是过程，更重要的是举一反三。该项目还引入了2个很有趣的Feature Importance Analysis的方法，对于我来说是新知识，也算是却到了。 这一篇到这里结束了，期待下一篇。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://jkknotes.com/categories/Machine-Learning/"},{"name":"项目实战","slug":"Machine-Learning/项目实战","permalink":"http://jkknotes.com/categories/Machine-Learning/项目实战/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://jkknotes.com/tags/Machine-Learning/"},{"name":"Random Forest","slug":"Random-Forest","permalink":"http://jkknotes.com/tags/Random-Forest/"},{"name":"AI医疗","slug":"AI医疗","permalink":"http://jkknotes.com/tags/AI医疗/"}]}]}